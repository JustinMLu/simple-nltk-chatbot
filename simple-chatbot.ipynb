{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db87cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03de164",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'intents.json') as data:\n",
    "    intents = json.loads(data.read())\n",
    "    \n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "# Tokenize all sentences in file \n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        token = nltk.word_tokenize(pattern)\n",
    "        words.extend(token)\n",
    "        classes.append(intent['tag'])\n",
    "        documents.append((token, intent['tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc8558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize all words \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ignored_words = ['?', '!']\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignored_words]\n",
    "\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple training data\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) \n",
    "    for word in pattern_words]\n",
    "    for w in words:\n",
    "        if w in pattern_words:\n",
    "            bag.append(1)\n",
    "        else: bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "X_train = list(training[:, 0])\n",
    "y_train = list(training[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49bb477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer = 128 neurons, second layer = 64, ReLU activation function for both \n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(y_train[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6811c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and train model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(np.array(X_train),\n",
    "                 np.array(y_train),\n",
    "                 epochs=200,\n",
    "                 batch_size=5,\n",
    "                 verbose=1)\n",
    "\n",
    "model.save('trained_model.h5', hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
